{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # might not be the best idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark packages \n",
    "from pyspark.sql.functions import col, concat, collect_list,struct\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import udf, array, array_distinct, array_min,array_max,array_union, explode\n",
    "from pyspark.sql.types import IntegerType, DoubleType, ArrayType\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of RDD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_rdd(file):\n",
    "    \"\"\"\n",
    "    This function takes a file name and converts it into an RDD.\n",
    "    \n",
    "    Arguments:\n",
    "    file (str): file name\n",
    "    \n",
    "    Returns:\n",
    "    An RDD containing all information extracted from the file.\n",
    "    \"\"\"\n",
    "\n",
    "    if file[-3:] == \"csv\" : \n",
    "        data = spark.read.format(\"csv\").option(\"inferSchema\", \"true\")\\\n",
    "                                        .option(\"delimiter\", ',')\\\n",
    "                                        .option(\"header\", 'true')\\\n",
    "                                        .load(file).cache()\n",
    "\n",
    "        rdd = adj_cache.rdd.map(tuple)\n",
    "        return rdd\n",
    "    elif file[-3:] == \"txt\" : \n",
    "        rdd_web = sc.textFile(file) \\\n",
    "                    .map(lambda line: line.split('\\t')) \\\n",
    "                    .filter(lambda line: len(line)>1) \\\n",
    "                        .map(lambda line: (line[0],line[1]))\n",
    "\n",
    "        return rdd_web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('abc').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter:  4\n",
      "counter:  9\n",
      "counter:  4\n",
      "counter:  0\n",
      "It took 7.684 seconds\n"
     ]
    }
   ],
   "source": [
    "#First try with the example of the research paper \n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "time1 = time.time()\n",
    "\n",
    "graph = sc.parallelize([(0, 1), (1, 2), (1, 3), (3, 4), (5, 6), (6, 7)])\n",
    "counter_new_pair = sc.accumulator(1)\n",
    "\n",
    "def reducer(node):\n",
    "    minimum = node[0]\n",
    "    value_List = []\n",
    "    joined = []\n",
    "  \n",
    "    for value in node[1]:\n",
    "        if value<minimum:\n",
    "            minimum = value\n",
    "        value_List.append(value)\n",
    "    if minimum<node[0]:\n",
    "        joined.append((node[0],minimum))\n",
    "        for value in value_List:\n",
    "            if minimum != value:\n",
    "                counter_new_pair.add(1)\n",
    "                joined.append((value, minimum))\n",
    "    return joined\n",
    "\n",
    "while counter_new_pair.value > 0:\n",
    "    counter_new_pair = sc.accumulator(0)\n",
    "\n",
    "    #CCF-Iterate\n",
    "    mapping_1 = graph.map(lambda node : (node[0], node[1]))\n",
    "    mapping_2 = graph.map(lambda node : (node[1], node[0]))\n",
    "    fusion = mapping_1.union(mapping_2)\n",
    "    fusion = fusion.groupByKey().map(lambda node : (node[0], list(node[1])))\n",
    "    joined = fusion.flatMap(lambda node: reducer(node))\n",
    "    # CCF-Dedup\n",
    "    graph = joined.map(lambda node : ((node[0], node[1]), None)).groupByKey()\n",
    "    graph = graph.map(lambda x: (x[0][0], x[0][1]))\n",
    "    graph.collect()\n",
    "  \n",
    "    print(\"counter: \", counter_new_pair)\n",
    "  \n",
    "    graph.collect()\n",
    "    \n",
    "time2 = time.time()\n",
    "\n",
    "print(\"It took {:.3f} seconds\".format(time2-time1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 16.0 failed 1 times, most recent failure: Lost task 2.0 in stage 16.0 (TID 82) (hippolytes-macbook-pro.home executor driver): java.io.IOException: Cannot run program \"python3.6\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:209)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:132)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:105)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:106)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:115)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 27 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: Cannot run program \"python3.6\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:209)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:132)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:105)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:106)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:115)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 27 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-fc1c48e80c64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"counter: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter_new_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \"\"\"\n\u001b[1;32m    948\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 16.0 failed 1 times, most recent failure: Lost task 2.0 in stage 16.0 (TID 82) (hippolytes-macbook-pro.home executor driver): java.io.IOException: Cannot run program \"python3.6\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:209)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:132)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:105)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:106)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:115)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 27 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: Cannot run program \"python3.6\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:209)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:132)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:105)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:106)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:115)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 27 more\n"
     ]
    }
   ],
   "source": [
    "#Let's try with the Google Graph \n",
    "\n",
    "web_google = sc.textFile(\"web-Google 2.txt\")\\\n",
    "                    .map(lambda line: line.split('\\t')) \\\n",
    "                    .filter(lambda line: len(line)>1) \\\n",
    "                        .map(lambda line: (line[0],line[1]))\n",
    "\n",
    "graph = web_google\n",
    "\n",
    "time1 = time.time()\n",
    "\n",
    "counter_new_pair = sc.accumulator(1)\n",
    "\n",
    "def reducer(node):\n",
    "    minimum = node[0]\n",
    "    value_List = []\n",
    "    joined = []\n",
    "  \n",
    "    for value in node[1]:\n",
    "        if value<minimum:\n",
    "            minimum = value\n",
    "        value_List.append(value)\n",
    "    if minimum<node[0]:\n",
    "        joined.append((node[0],minimum))\n",
    "        for value in value_List:\n",
    "            if minimum != value:\n",
    "                counter_new_pair.add(1)\n",
    "                joined.append((value, minimum))\n",
    "    return joined\n",
    "\n",
    "while counter_new_pair.value > 0:\n",
    "    counter_new_pair = sc.accumulator(0)\n",
    "\n",
    "    #CCF-Iterate\n",
    "    mapping_1 = graph.map(lambda node : (node[0], node[1]))\n",
    "    mapping_2 = graph.map(lambda node : (node[1], node[0]))\n",
    "    fusion = mapping_1.union(mapping_2)\n",
    "    fusion = fusion.groupByKey().map(lambda node : (node[0], list(node[1])))\n",
    "    joined = fusion.flatMap(lambda node: reducer(node))\n",
    "    # CCF-Dedup\n",
    "    graph = joined.map(lambda node : ((node[0], node[1]), None)).groupByKey()\n",
    "    graph = graph.map(lambda x: (x[0][0], x[0][1]))\n",
    "    graph.collect()\n",
    "  \n",
    "    print(\"counter: \", counter_new_pair)\n",
    "    \n",
    "time2 = time.time()\n",
    "\n",
    "print(\"It took {:.3f} seconds\".format(time2-time1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison according to graph size \n",
    "\n",
    "values = [len(web_google.collect())//(10-i) for i in range(1, 10)]\n",
    "\n",
    "time_ = []\n",
    "\n",
    "for i in range(len(values)):\n",
    "    graph = sc.parallelize(web_google.collect()[1:values[i]])\n",
    "    \n",
    "    time1 = time.time()\n",
    "    \n",
    "    counter_new_pair = sc.accumulator(1)\n",
    "    \n",
    "    while counter_new_pair.value > 0:\n",
    "        \n",
    "        counter_new_pair = sc.accumulator(0)\n",
    "\n",
    "        #CCF-Iterate\n",
    "        mapping_1 = graph.map(lambda node : (node[0], node[1]))\n",
    "        mapping_2 = graph.map(lambda node : (node[1], node[0]))\n",
    "        fusion = mapping_1.union(mapping_2)\n",
    "        fusion = fusion.groupByKey().map(lambda node : (node[0], list(node[1])))\n",
    "        joined = fusion.flatMap(lambda node: reducer(node))\n",
    "        # CCF-Dedup\n",
    "        graph = joined.map(lambda node : ((node[0], node[1]), None)).groupByKey()\n",
    "        graph = graph.map(lambda x: (x[0][0], x[0][1]))\n",
    "        graph.collect()\n",
    "    \n",
    "    time2 = time.time()\n",
    "    \n",
    "    time_.append((i, time2-time1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Speed (in seconds) relatively to the % of the graph taken\")\n",
    "plt.plot([time_[i][1] for i in range(len(time_))]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try part, Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"inferSchema\", \"true\")\\\n",
    "                                        .option(\"delimiter\", ',')\\\n",
    "                                        .option(\"header\", 'true')\\\n",
    "                                        .load(\"web-Google_total.csv\").toDF(\"val\",\"key\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|   val|   key|\n",
      "+------+------+\n",
      "|     0|867923|\n",
      "|     0|891835|\n",
      "| 11342|     0|\n",
      "| 11342| 27469|\n",
      "| 11342| 38716|\n",
      "| 11342|309564|\n",
      "| 11342|322178|\n",
      "| 11342|387543|\n",
      "| 11342|427436|\n",
      "| 11342|538214|\n",
      "| 11342|638706|\n",
      "| 11342|645018|\n",
      "| 11342|835220|\n",
      "| 11342|856657|\n",
      "| 11342|867923|\n",
      "| 11342|891835|\n",
      "|824020|     0|\n",
      "|824020| 91807|\n",
      "|824020|322178|\n",
      "|824020|387543|\n",
      "+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8275872\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "def ccf_iterate_map_df(df):\n",
    "    newRow = df.select(\"val\", \"key\")\n",
    "    df1 = df.union(newRow)\n",
    "    return df1\n",
    "\n",
    "def ccf_iterate_reduce_df(df1):\n",
    "    window = Window.orderBy(\"key\",\"val\").partitionBy(\"key\")\n",
    "    df_min = df1.withColumn(\"min\", min(\"val\").over(window))\n",
    "\n",
    "    new_column_1 = expr( \"\"\"IF(min > key, Null, IF(min = val, key, val))\"\"\")\n",
    "    new_column_2 = expr(\"\"\"IF(min > key, Null, min)\"\"\")\n",
    "    new_df = (df_min\n",
    "      .withColumn(\"new_key\", new_column_1)\n",
    "      .withColumn(\"new_val\", new_column_2)) \\\n",
    "    .na.drop() \\\n",
    "    .select(col(\"new_key\").alias(\"key\"), col(\"new_val\").alias(\"val\")) \\\n",
    "    .sort(\"val\", \"key\") \n",
    "        \n",
    "    df2 = new_df.distinct()\n",
    "    \n",
    "    return df2, df_min\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "begin = time.time()\n",
    "counter = 1\n",
    "iteration = 0\n",
    "while counter!=0:\n",
    "    iteration +=1\n",
    "    df1 = ccf_iterate_map_df(df)\n",
    "    df1.cache()\n",
    "    df.unpersist()\n",
    "    df, df_counter = ccf_iterate_reduce_df(df1)\n",
    "    df.cache()\n",
    "    df1.unpersist()\n",
    "    df_counter = df_counter.withColumn(\"counter_col\", expr(\"\"\"IF(min > key, 0, IF(min = val, 0, 1))\"\"\"))\n",
    "    counter = df_counter.select(sum(\"counter_col\")).collect()[0][0]\n",
    "    print(counter)\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645720\n",
      "188263\n",
      "249888\n",
      "326246\n",
      "252536\n",
      "73441\n",
      "518\n",
      "0\n",
      "('web-Google_500000.csv', 476.46466517448425)\n",
      "1379336\n",
      "411275\n",
      "466760\n",
      "634325\n",
      "483907\n",
      "133761\n",
      "1344\n",
      "0\n",
      "('web-Google_1000000.csv', 543.9730443954468)\n",
      "2158798\n",
      "651571\n",
      "663595\n",
      "916646\n",
      "680213\n",
      "154277\n",
      "1966\n",
      "0\n",
      "('web-Google_1500000.csv', 571.6710729598999)\n",
      "2965172\n",
      "912104\n",
      "846662\n",
      "1189233\n",
      "876542\n",
      "174605\n",
      "2038\n",
      "0\n",
      "('web-Google_2000000.csv', 583.3043549060822)\n",
      "3789920\n",
      "1184366\n",
      "1023125\n",
      "1451456\n",
      "1057318\n",
      "205858\n",
      "2436\n",
      "0\n",
      "('web-Google_2500000.csv', 596.1450490951538)\n",
      "4630528\n",
      "1479228\n",
      "1199555\n",
      "1712234\n",
      "1248788\n",
      "214513\n",
      "3162\n",
      "0\n",
      "('web-Google_3000000.csv', 632.7033257484436)\n",
      "5483928\n",
      "1785818\n",
      "1373705\n",
      "1981569\n",
      "1446347\n",
      "232332\n",
      "4126\n",
      "10\n",
      "0\n",
      "('web-Google_3500000.csv', 830.4311218261719)\n",
      "6343536\n",
      "2102129\n",
      "1548923\n",
      "2262060\n",
      "1654681\n",
      "257297\n",
      "5834\n",
      "22\n",
      "0\n",
      "('web-Google_4000000.csv', 929.0334510803223)\n",
      "8275872\n",
      "2772256\n",
      "1919694\n",
      "2879945\n",
      "2146951\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o5281.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 43 in stage 967.0 failed 1 times, most recent failure: Lost task 43.0 in stage 967.0 (TID 205085) (hippolytes-macbook-pro.home executor driver): org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 262144 bytes of memory, got 0\n\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:157)\n\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:97)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap.allocate(BytesToBytesMap.java:870)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap.reset(BytesToBytesMap.java:974)\n\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.<init>(UnsafeKVExternalSorter.java:173)\n\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.destructAndCreateExternalSorter(UnsafeFixedWidthAggregationMap.java:248)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage124.agg_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage124.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage124.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage124.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\n\tat sun.reflect.GeneratedMethodAccessor171.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 262144 bytes of memory, got 0\n\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:157)\n\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:97)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap.allocate(BytesToBytesMap.java:870)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap.reset(BytesToBytesMap.java:974)\n\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.<init>(UnsafeKVExternalSorter.java:173)\n\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.destructAndCreateExternalSorter(UnsafeFixedWidthAggregationMap.java:248)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage124.agg_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage124.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage124.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage124.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-60de216b107c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mdf_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"counter_col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\"IF(min > key, 0, IF(min = val, 0, 1))\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"counter_col\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \"\"\"\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o5281.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 43 in stage 967.0 failed 1 times, most recent failure: Lost task 43.0 in stage 967.0 (TID 205085) (hippolytes-macbook-pro.home executor driver): org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 262144 bytes of memory, got 0\n\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:157)\n\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:97)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap.allocate(BytesToBytesMap.java:870)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap.reset(BytesToBytesMap.java:974)\n\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.<init>(UnsafeKVExternalSorter.java:173)\n\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.destructAndCreateExternalSorter(UnsafeFixedWidthAggregationMap.java:248)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage124.agg_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage124.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage124.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage124.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\n\tat sun.reflect.GeneratedMethodAccessor171.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 262144 bytes of memory, got 0\n\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:157)\n\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:97)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap.allocate(BytesToBytesMap.java:870)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap.reset(BytesToBytesMap.java:974)\n\tat org.apache.spark.sql.execution.UnsafeKVExternalSorter.<init>(UnsafeKVExternalSorter.java:173)\n\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.destructAndCreateExternalSorter(UnsafeFixedWidthAggregationMap.java:248)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage124.agg_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage124.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage124.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage124.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "names = [\"web-Google_500000.csv\", \"web-Google_1000000.csv\", \"web-Google_1500000.csv\", \"web-Google_2000000.csv\", \"web-Google_2500000.csv\", \"web-Google_3000000.csv\", \"web-Google_3500000.csv\", \"web-Google_4000000.csv\", \"web-Google_total.csv\"]\n",
    "\n",
    "liste = []\n",
    "for i in names:\n",
    "    df = spark.read.format(\"csv\").option(\"inferSchema\", \"true\")\\\n",
    "                                        .option(\"delimiter\", ',')\\\n",
    "                                        .option(\"header\", 'true')\\\n",
    "                                        .load(i).toDF(\"val\",\"key\").cache()\n",
    "    begin = time.time()\n",
    "    counter = 1\n",
    "    iteration = 0\n",
    "    while counter!=0:\n",
    "        iteration +=1\n",
    "        df1 = ccf_iterate_map_df(df)\n",
    "        df1.cache()\n",
    "        df.unpersist()\n",
    "        df, df_counter = ccf_iterate_reduce_df(df1)\n",
    "        df.cache()\n",
    "        df1.unpersist()\n",
    "        df_counter = df_counter.withColumn(\"counter_col\", expr(\"\"\"IF(min > key, 0, IF(min = val, 0, 1))\"\"\"))\n",
    "        counter = df_counter.select(sum(\"counter_col\")).collect()[0][0]\n",
    "        print(counter)\n",
    "    \n",
    "    end = time.time()\n",
    "    print((i, end-begin))\n",
    "    liste.append((i, end-begin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sc.textFile(\"web-Google 2.txt\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sc.parallelize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.map(lambda line: line.split('\\t')) \\\n",
    "                    .filter(lambda line: len(line)>1) \\\n",
    "                        .map(lambda line: (line[0],line[1])).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8276304\n",
      "2765183\n",
      "1913801\n",
      "2880537\n",
      "2143683\n",
      "296831\n",
      "8504\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1154.unpersist.\n: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7baf3f16f219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mccf_iterate_reduce_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mdf_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"counter_col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\"IF(min > _2, 0, IF(min = _1, 0, 1))\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mcounter_new_pair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"counter_col\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36munpersist\u001b[0;34m(self, blocking)\u001b[0m\n\u001b[1;32m    841\u001b[0m         \"\"\"\n\u001b[1;32m    842\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1154.unpersist.\n: java.lang.OutOfMemoryError: GC overhead limit exceeded\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = sc.textFile(\"web-Google 2.txt\").collect()\n",
    "df = sc.parallelize(df)\n",
    "df = df.map(lambda line: line.split('\\t')) \\\n",
    "                    .filter(lambda line: len(line)>1) \\\n",
    "                        .map(lambda line: (line[0],line[1])).toDF()\n",
    "\n",
    "def ccf_iterate_map_df(df):\n",
    "    newRow = df.select(\"_1\", \"_2\")\n",
    "    df1 = df.union(newRow)\n",
    "    return df1\n",
    "\n",
    "def ccf_iterate_reduce_df(df1):\n",
    "    window = Window.orderBy(\"_2\",\"_1\").partitionBy(\"_2\")\n",
    "    df_min = df1.withColumn(\"min\", min(\"_1\").over(window))\n",
    "\n",
    "    new_column_1 = expr( \"\"\"IF(min > _2, Null, IF(min = _1, _2, _1))\"\"\")\n",
    "    new_column_2 = expr(\"\"\"IF(min > _2, Null, min)\"\"\")\n",
    "    new_df = (df_min\n",
    "      .withColumn(\"new_key\", new_column_1)\n",
    "      .withColumn(\"new_val\", new_column_2)) \\\n",
    "    .na.drop() \\\n",
    "    .select(col(\"new_key\").alias(\"_2\"), col(\"new_val\").alias(\"_1\")) \\\n",
    "    .sort(\"_1\", \"_2\") \n",
    "        \n",
    "    df2 = new_df.distinct()\n",
    "    \n",
    "    return df2, df_min\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "begin = time.time()\n",
    "counter_new_pair = sc.accumulator(1)\n",
    "iteration = 0\n",
    "while counter_new_pair!=0:\n",
    "    counter_new_pair = sc.accumulator(0)\n",
    "    iteration +=1\n",
    "    df1 = ccf_iterate_map_df(df)\n",
    "    df1.cache()\n",
    "    df.unpersist()\n",
    "    df, df_counter = ccf_iterate_reduce_df(df1)\n",
    "    df.cache()\n",
    "    df1.unpersist()\n",
    "    df_counter = df_counter.withColumn(\"counter_col\", expr(\"\"\"IF(min > _2, 0, IF(min = _1, 0, 1))\"\"\"))\n",
    "    counter_new_pair = sc.accumulator(df_counter.select(sum(\"counter_col\")).collect()[0][0])\n",
    "    if counter_new_pair==0:\n",
    "        end = time.time()\n",
    "        print(end-begin)\n",
    "        break\n",
    "    print(counter_new_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "861.835107088089\n"
     ]
    }
   ],
   "source": [
    "print(end-begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`key`' given input columns: [_1, _2, min]; line 1 pos 9;\n'Project [_1#220960, _2#220961, min#221000, 'IF((min#221000 > 'key), 0, 'IF((min#221000 = 'val), 0, 1)) AS counter_col#221074]\n+- Project [_1#220960, _2#220961, min#221000]\n   +- Project [_1#220960, _2#220961, min#221000, min#221000]\n      +- Window [min(_2#220961) windowspecdefinition(_2#220961, _2#220961 ASC NULLS FIRST, _1#220960 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS min#221000], [_2#220961], [_2#220961 ASC NULLS FIRST, _1#220960 ASC NULLS FIRST]\n         +- Project [_1#220960, _2#220961]\n            +- Union false, false\n               :- LogicalRDD [_1#220960, _2#220961], false\n               +- Project [_1#220960 AS _1#220975, _2#220961 AS _2#220976]\n                  +- Project [_1#220960, _2#220961]\n                     +- LogicalRDD [_1#220960, _2#220961], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-f8d25be6512c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mdf_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"counter_col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\"IF(min > key, 0, IF(min = val, 0, 1))\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"counter_col\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2453\u001b[0m         \"\"\"\n\u001b[1;32m   2454\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`key`' given input columns: [_1, _2, min]; line 1 pos 9;\n'Project [_1#220960, _2#220961, min#221000, 'IF((min#221000 > 'key), 0, 'IF((min#221000 = 'val), 0, 1)) AS counter_col#221074]\n+- Project [_1#220960, _2#220961, min#221000]\n   +- Project [_1#220960, _2#220961, min#221000, min#221000]\n      +- Window [min(_2#220961) windowspecdefinition(_2#220961, _2#220961 ASC NULLS FIRST, _1#220960 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS min#221000], [_2#220961], [_2#220961 ASC NULLS FIRST, _1#220960 ASC NULLS FIRST]\n         +- Project [_1#220960, _2#220961]\n            +- Union false, false\n               :- LogicalRDD [_1#220960, _2#220961], false\n               +- Project [_1#220960 AS _1#220975, _2#220961 AS _2#220976]\n                  +- Project [_1#220960, _2#220961]\n                     +- LogicalRDD [_1#220960, _2#220961], false\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "def ccf_iterate_map_df(df):\n",
    "    newRow = df.select(\"_1\", \"_2\")\n",
    "    df1 = df.union(newRow)\n",
    "    return df1\n",
    "\n",
    "def ccf_iterate_reduce_df(df1):\n",
    "    window = Window.orderBy(\"_2\",\"_1\").partitionBy(\"_2\")\n",
    "    df_min = df1.withColumn(\"min\", min(\"_2\").over(window))\n",
    "\n",
    "    new_column_1 = expr( \"\"\"IF(min > _2, Null, IF(min = _1, _2, _1))\"\"\")\n",
    "    new_column_2 = expr(\"\"\"IF(min > _2, Null, min)\"\"\")\n",
    "    new_df = (df_min\n",
    "      .withColumn(\"new_key\", new_column_1)\n",
    "      .withColumn(\"new_val\", new_column_2)) \\\n",
    "    .na.drop() \\\n",
    "    .select(col(\"new_key\").alias(\"_2\"), col(\"new_val\").alias(\"_1\")) \\\n",
    "    .sort(\"_1\", \"_2\") \n",
    "        \n",
    "    df2 = new_df.distinct()\n",
    "    \n",
    "    return df2, df_min\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "begin = time.time()\n",
    "counter = 1\n",
    "iteration = 0\n",
    "while counter!=0:\n",
    "    iteration +=1\n",
    "    df1 = ccf_iterate_map_df(df)\n",
    "    df1.cache()\n",
    "    df.unpersist()\n",
    "    df, df_counter = ccf_iterate_reduce_df(df1)\n",
    "    df.cache()\n",
    "    df1.unpersist()\n",
    "    df_counter = df_counter.withColumn(\"counter_col\", expr(\"\"\"IF(min > key, 0, IF(min = val, 0, 1))\"\"\"))\n",
    "    counter = df_counter.select(sum(\"counter_col\")).collect()[0][0]\n",
    "    print(counter)\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverseDF = df.select(col(\"From\").alias(\"To\"),col(\"To\").alias(\"From\"))# getting all (v,k)\n",
    "    df_0 = df.union(reverseDF)# Building a new DataFrame containing all (k,v) and (v,k)\n",
    "    \n",
    "    # Grouping by key on the first element (k, [v1, v2...])\n",
    "    df_1 = df_0.groupBy(col(\"To\")).agg(f.array_distinct(collect_list(col(\"From\"))).alias('From'))\n",
    "    \n",
    "    # New k: the minimum between k and all elements included in v \n",
    "    # New v: all values from k and v\n",
    "    df_2 = df_1.withColumn('From', array_union(df_1.From, array(df_1.To))).withColumn('To', array_min(\"From\"))\n",
    "    \n",
    "    # Extracting each element of v as our key k and assigning it the corresponding minimum found above\n",
    "    df_3 = df_2.select( explode(col(\"From\")).alias(\"To\"), col(\"To\").alias(\"From\")).dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|    To|  From|\n",
      "+------+------+\n",
      "|     0|867923|\n",
      "|     0|891835|\n",
      "| 11342|     0|\n",
      "| 11342| 27469|\n",
      "| 11342| 38716|\n",
      "| 11342|309564|\n",
      "| 11342|322178|\n",
      "| 11342|387543|\n",
      "| 11342|427436|\n",
      "| 11342|538214|\n",
      "| 11342|638706|\n",
      "| 11342|645018|\n",
      "| 11342|835220|\n",
      "| 11342|856657|\n",
      "| 11342|867923|\n",
      "| 11342|891835|\n",
      "|824020|     0|\n",
      "|824020| 91807|\n",
      "|824020|322178|\n",
      "|824020|387543|\n",
      "+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, let's define our function to import and analyze Dataframes\n",
    "\n",
    "def our_union (x):\n",
    "    x[1].append(x[0])\n",
    "    return x[1]\n",
    "\n",
    "our_union_udf = f.udf(our_union, ArrayType(IntegerType()))\n",
    "findmin = f.udf(lambda x: min(x), IntegerType())\n",
    "our_distinct = f.udf(lambda x: list(set(x)), ArrayType(IntegerType()))\n",
    "\n",
    "def CCF_DEDUP_df(df):\n",
    "    \n",
    "    reverseDF = df.select(col(\"From\").alias(\"To\"),col(\"To\").alias(\"From\"))\n",
    "    df_0 = df.union(reverseDF)\n",
    "    \n",
    "    df_1 = df_0.groupBy(col(\"To\")).agg(our_distinct(collect_list(col(\"From\"))).alias('From'))\n",
    "    df_2 = df_1.withColumn('From', our_union_udf(struct(df_1.To, df_1.From)))\\\n",
    "                    .withColumn('To', findmin(\"From\"))\\\n",
    "                        .withColumn('From', our_distinct('From'))\n",
    "    \n",
    "    df_3 = df_2.select( explode(col(\"From\")).alias(\"To\"), col(\"To\").alias(\"From\")).dropDuplicates()\n",
    "    \n",
    "    return df_3\n",
    "\n",
    "def Analyze(df):\n",
    "    \n",
    "    reverseDF = df.select(col(\"From\").alias(\"To\"),col(\"To\").alias(\"From\"))\n",
    "    df_0 = df.union(reverseDF)\n",
    "      \n",
    "    size = df_0.distinct().count()/2\n",
    "\n",
    "    t = time.time()\n",
    "    \n",
    "    counter = 0 \n",
    "    while df.count()!= df.select('To').distinct().count() :\n",
    "        counter +=1 \n",
    "        df = CCF_DEDUP_df(df) \n",
    "    t = time.time() - t\n",
    "    \n",
    "    #Getting the number of groups of connected components\n",
    "    num_of_groups = len(df.select('From').distinct().collect())\n",
    "    \n",
    "    return t, size,num_of_groups, counter,  df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'select'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-75da638cbeea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mAnalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"web-Google_total.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-affca6b78e1e>\u001b[0m in \u001b[0;36mAnalyze\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mAnalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mreverseDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"From\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"To\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"To\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"From\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mdf_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreverseDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'select'"
     ]
    }
   ],
   "source": [
    "Analyze(\"web-Google_total.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1 = pd.read_csv(\"web-Google 2.txt\", sep=\"\\t\").head(5000000000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1.to_csv(\"web-Google_test.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Analyze' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-8c51a8bf3d5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                         \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"To\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"From\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtime_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAnalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Analyze' is not defined"
     ]
    }
   ],
   "source": [
    "names = [\"web-Google_500000.csv\", \"web-Google_1000000.csv\", \"web-Google_1500000.csv\", \"web-Google_2000000.csv\", \"web-Google_2500000.csv\", \"web-Google_3000000.csv\", \"web-Google_3500000.csv\", \"web-Google_4000000.csv\", \"web-Google_1000000.csv\"]\n",
    "\n",
    "time_df = []\n",
    "\n",
    "for file in names:\n",
    "    df = spark.read.format(\"csv\").option(\"inferSchema\", \"true\")\\\n",
    "                                        .option(\"delimiter\", ',')\\\n",
    "                                        .option(\"header\", 'true')\\\n",
    "                                        .load(file).toDF(\"To\",\"From\").cache()\n",
    "    \n",
    "    time_df.append(Analyze(df)[0])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = [742.6663370132446,\n",
    " 1173.5141010284424,\n",
    " 1178.273682832718,\n",
    " 1244.8001997470856,\n",
    " 2275.856563806534,\n",
    " 1476.6470799446106,\n",
    " 1604.9051089286804,\n",
    " 1770.9568598270416,\n",
    " 1165.5193858146667]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD vs Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Speed (in seconds) relatively to the % of the graph taken\")\n",
    "plt.plot([time_[i][1] for i in range(len(time_))], label=\"RDD\")\n",
    "plt.plot([time_df[i] for i in range(len(time_df))], label=\"DF\")\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:50245)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hippolyteguigon/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hippolyteguigon/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 61] Connection refused\n"
     ]
    },
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server (127.0.0.1:50245)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-4fca77a6fbbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inferSchema\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m                                         \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delimiter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                         \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                         \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"web-Google 2.txt.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"To\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"From\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataFrameReader\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \"\"\"\n\u001b[0;32m--> 755\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrameReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spark)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1029\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \"\"\"\n\u001b[0;32m-> 1031\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    983\u001b[0m         connection = GatewayConnection(\n\u001b[1;32m    984\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[0;32m--> 985\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1125\u001b[0m                 \u001b[0;34m\"server ({0}:{1})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_authenticate_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:50245)"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option(\"inferSchema\", \"true\")\\\n",
    "                                        .option(\"delimiter\", ',')\\\n",
    "                                        .option(\"header\", 'true')\\\n",
    "                                        .load(\"web-Google 2.txt.csv\").toDF(\"To\",\"From\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499999"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-1adec6a6e8af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                         .load(i).toDF(\"To\",\"From\").cache().count() + 1 for i in names]\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m computation.loc[\"Number of connected components\"] = [Analyze(spark.read.format(\"csv\").option(\"inferSchema\", \"true\")\\\n\u001b[0m\u001b[1;32m     13\u001b[0m                                         \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delimiter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                         \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-1adec6a6e8af>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m                                         .load(i).toDF(\"To\",\"From\").cache().count() + 1 for i in names]\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m computation.loc[\"Number of connected components\"] = [Analyze(spark.read.format(\"csv\").option(\"inferSchema\", \"true\")\\\n\u001b[0m\u001b[1;32m     13\u001b[0m                                         \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delimiter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                         \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-affca6b78e1e>\u001b[0m in \u001b[0;36mAnalyze\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m!=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'To'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCCF_DEDUP_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \"\"\"\n\u001b[0;32m--> 664\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "names = [\"web-Google_500000.csv\", \"web-Google_1000000.csv\", \"web-Google_1500000.csv\", \"web-Google_2000000.csv\", \"web-Google_2500000.csv\", \"web-Google_3000000.csv\", \"web-Google_3500000.csv\", \"web-Google_4000000.csv\", \"web-Google_total.csv\"]\n",
    "\n",
    "computation = pd.DataFrame(columns=names)\n",
    "\n",
    "time1 = time.time()\n",
    "\n",
    "computation.loc[\"Number of elements\"] = [spark.read.format(\"csv\").option(\"inferSchema\", \"true\")\\\n",
    "                                        .option(\"delimiter\", ',')\\\n",
    "                                        .option(\"header\", 'true')\\\n",
    "                                        .load(i).toDF(\"To\",\"From\").cache().count() + 1 for i in names]\n",
    "\n",
    "computation.loc[\"Number of connected components\"] = [Analyze(spark.read.format(\"csv\").option(\"inferSchema\", \"true\")\\\n",
    "                                        .option(\"delimiter\", ',')\\\n",
    "                                        .option(\"header\", 'true')\\\n",
    "                                        .load(i).toDF(\"To\",\"From\").cache())[2] for i in names]\n",
    "\n",
    "computation.loc[\"Computation time\"] = [Analyze(spark.read.format(\"csv\").option(\"inferSchema\", \"true\")\\\n",
    "                                        .option(\"delimiter\", ',')\\\n",
    "                                        .option(\"header\", 'true')\\\n",
    "                                        .load(i).toDF(\"To\",\"From\").cache())[3] for i in names]\n",
    "\n",
    "time2 = time.time()\n",
    "\n",
    "print(\"It took {:.2f} seconds for this operation\".format(time2-time1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`val`' given input columns: [From, To];\n'Project [To#175938, From#175939, min('val) windowspecdefinition('key, 'key ASC NULLS FIRST, 'val ASC NULLS FIRST, unspecifiedframe$()) AS min#175978]\n+- Union false, false\n   :- Project [cast(_1#175934 as int) AS To#175938, cast(_2#175935 as int) AS From#175939]\n   :  +- LogicalRDD [_1#175934, _2#175935], false\n   +- Project [To#175938 AS To#175953, From#175939 AS From#175954]\n      +- Project [To#175938, From#175939]\n         +- Project [cast(_1#175934 as int) AS To#175938, cast(_2#175935 as int) AS From#175939]\n            +- LogicalRDD [_1#175934, _2#175935], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-773f3afc65ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mccf_iterate_reduce_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-773f3afc65ba>\u001b[0m in \u001b[0;36mccf_iterate_reduce_df\u001b[0;34m(df1)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mccf_iterate_reduce_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"key\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdf_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"min\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mnew_column_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"\"\"IF(min > key, Null, IF(min = val, key, val))\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2453\u001b[0m         \"\"\"\n\u001b[1;32m   2454\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`val`' given input columns: [From, To];\n'Project [To#175938, From#175939, min('val) windowspecdefinition('key, 'key ASC NULLS FIRST, 'val ASC NULLS FIRST, unspecifiedframe$()) AS min#175978]\n+- Union false, false\n   :- Project [cast(_1#175934 as int) AS To#175938, cast(_2#175935 as int) AS From#175939]\n   :  +- LogicalRDD [_1#175934, _2#175935], false\n   +- Project [To#175938 AS To#175953, From#175939 AS From#175954]\n      +- Project [To#175938, From#175939]\n         +- Project [cast(_1#175934 as int) AS To#175938, cast(_2#175935 as int) AS From#175939]\n            +- LogicalRDD [_1#175934, _2#175935], false\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "def ccf_iterate_map_df(df):\n",
    "    newRow = df.select(\"val\", \"key\")\n",
    "    df1 = df.union(newRow)\n",
    "    return df1\n",
    "\n",
    "def ccf_iterate_reduce_df(df1):\n",
    "    window = Window.orderBy(\"key\",\"val\").partitionBy(\"key\")\n",
    "    df_min = df1.withColumn(\"min\", min(\"val\").over(window))\n",
    "\n",
    "    new_column_1 = expr( \"\"\"IF(min > key, Null, IF(min = val, key, val))\"\"\")\n",
    "    new_column_2 = expr(\"\"\"IF(min > key, Null, min)\"\"\")\n",
    "    new_df = (df_min\n",
    "      .withColumn(\"new_key\", new_column_1)\n",
    "      .withColumn(\"new_val\", new_column_2)) \\\n",
    "    .na.drop() \\\n",
    "    .select(col(\"new_key\").alias(\"key\"), col(\"new_val\").alias(\"val\")) \\\n",
    "    .sort(\"val\", \"key\") \n",
    "        \n",
    "    df2 = new_df.distinct()\n",
    "    \n",
    "    return df2, df_min\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "begin = time.time()\n",
    "counter = 1\n",
    "iteration = 0\n",
    "while counter!=0:\n",
    "    iteration +=1\n",
    "    df1 = ccf_iterate_map_df(df)\n",
    "    df1.cache()\n",
    "    df.unpersist()\n",
    "    df, df_counter = ccf_iterate_reduce_df(df1)\n",
    "    df.cache()\n",
    "    df1.unpersist()\n",
    "    df_counter = df_counter.withColumn(\"counter_col\", expr(\"\"\"IF(min > key, 0, IF(min = val, 0, 1))\"\"\"))\n",
    "    counter = df_counter.select(sum(\"counter_col\")).collect()[0][0]\n",
    "    print(counter)\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:50245)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hippolyteguigon/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hippolyteguigon/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 61] Connection refused\n"
     ]
    },
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server (127.0.0.1:50245)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-5e2f5e5ba14c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweb_google\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweb_google\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    946\u001b[0m         \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0msmall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mall\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0minto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         \"\"\"\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/traceback_utils.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark_stack_depth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_site\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark_stack_depth\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1029\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \"\"\"\n\u001b[0;32m-> 1031\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    983\u001b[0m         connection = GatewayConnection(\n\u001b[1;32m    984\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[0;32m--> 985\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1125\u001b[0m                 \u001b[0;34m\"server ({0}:{1})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_authenticate_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:50245)"
     ]
    }
   ],
   "source": [
    "graph = sc.parallelize(web_google.collect()[1:len(web_google.collect())])\n",
    "graph = graph.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "can't set attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-291ebf5bfb76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"From\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"To\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: can't set attribute"
     ]
    }
   ],
   "source": [
    "graph.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_union (x):\n",
    "    x[1].append(x[0])\n",
    "    return x[1]\n",
    "\n",
    "our_union_udf = f.udf(our_union, ArrayType(IntegerType()))\n",
    "findmin = f.udf(lambda x: min(x), IntegerType())\n",
    "our_distinct = f.udf(lambda x: list(set(x)), ArrayType(IntegerType()))\n",
    "\n",
    "def CCF_DEDUP_df(df):\n",
    "    \n",
    "    reverseDF = df.select(col(\"From\").alias(\"To\"),col(\"To\").alias(\"From\"))\n",
    "    df_0 = df.union(reverseDF)\n",
    "    \n",
    "    df_1 = df_0.groupBy(col(\"To\")).agg(our_distinct(collect_list(col(\"From\"))).alias('From'))\n",
    "    df_2 = df_1.withColumn('From', our_union_udf(struct(df_1.To, df_1.From)))\\\n",
    "                    .withColumn('To', findmin(\"From\"))\\\n",
    "                        .withColumn('From', our_distinct('From'))\n",
    "    \n",
    "    df_3 = df_2.select( explode(col(\"From\")).alias(\"To\"), col(\"To\").alias(\"From\")).dropDuplicates()\n",
    "    \n",
    "    return df_3\n",
    "\n",
    "def Analyze(df):\n",
    "    \n",
    "    reverseDF = df.select(col(\"From\").alias(\"To\"),col(\"To\").alias(\"From\"))\n",
    "    df_0 = df.union(reverseDF)\n",
    "      \n",
    "    size = df_0.distinct().count()/2\n",
    "\n",
    "    t = time.time()\n",
    "    \n",
    "    counter = sc.accumulator(1)\n",
    "    while df.count()!= df.select('To').distinct().count() :\n",
    "        counter_new_pair.add(1)\n",
    "        df = CCF_DEDUP_df(df) \n",
    "    t = time.time() - t\n",
    "    \n",
    "    #Getting the number of groups of connected components\n",
    "    num_of_groups = len(df.select('From').distinct().collect())\n",
    "    \n",
    "    return t, size,num_of_groups, counter,  df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`key`' given input columns: [From, To, min]; line 1 pos 9;\n'Project [To#521, From#522, min#663, 'IF((min#663 > 'key), null, 'IF((min#663 = 'val), 'key, 'val)) AS new_key#667]\n+- Project [To#521, From#522, min#663]\n   +- Project [To#521, From#522, min#663, min#663]\n      +- Window [min(From#522) windowspecdefinition(To#521, To#521 ASC NULLS FIRST, From#522 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS min#663], [To#521], [To#521 ASC NULLS FIRST, From#522 ASC NULLS FIRST]\n         +- Project [To#521, From#522]\n            +- Union false, false\n               :- Project [0#517 AS To#521, 824020#518 AS From#522]\n               :  +- Relation[0#517,824020#518] csv\n               +- Project [From#522 AS From#578, To#521 AS To#579]\n                  +- Project [From#522, To#521]\n                     +- Project [0#517 AS To#521, 824020#518 AS From#522]\n                        +- Relation[0#517,824020#518] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-d893be0c88a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mccf_iterate_reduce_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-d893be0c88a2>\u001b[0m in \u001b[0;36mccf_iterate_reduce_df\u001b[0;34m(df1)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mnew_column_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"\"\"IF(min > key, Null, IF(min = val, key, val))\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mnew_column_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\"IF(min > key, Null, min)\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     new_df = (df_min\n\u001b[0m\u001b[1;32m     17\u001b[0m       \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new_key\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_column_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       .withColumn(\"new_val\", new_column_2)) \\\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2453\u001b[0m         \"\"\"\n\u001b[1;32m   2454\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`key`' given input columns: [From, To, min]; line 1 pos 9;\n'Project [To#521, From#522, min#663, 'IF((min#663 > 'key), null, 'IF((min#663 = 'val), 'key, 'val)) AS new_key#667]\n+- Project [To#521, From#522, min#663]\n   +- Project [To#521, From#522, min#663, min#663]\n      +- Window [min(From#522) windowspecdefinition(To#521, To#521 ASC NULLS FIRST, From#522 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS min#663], [To#521], [To#521 ASC NULLS FIRST, From#522 ASC NULLS FIRST]\n         +- Project [To#521, From#522]\n            +- Union false, false\n               :- Project [0#517 AS To#521, 824020#518 AS From#522]\n               :  +- Relation[0#517,824020#518] csv\n               +- Project [From#522 AS From#578, To#521 AS To#579]\n                  +- Project [From#522, To#521]\n                     +- Project [0#517 AS To#521, 824020#518 AS From#522]\n                        +- Relation[0#517,824020#518] csv\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "def ccf_iterate_map_df(df):\n",
    "    newRow = df.select(\"val\", \"key\")\n",
    "    df1 = df.union(newRow)\n",
    "    return df1\n",
    "\n",
    "def ccf_iterate_reduce_df(df1):\n",
    "    window = Window.orderBy(\"key\",\"val\").partitionBy(\"key\")\n",
    "    df_min = df1.withColumn(\"min\", min(\"val\").over(window))\n",
    "\n",
    "    new_column_1 = expr( \"\"\"IF(min > key, Null, IF(min = val, key, val))\"\"\")\n",
    "    new_column_2 = expr(\"\"\"IF(min > key, Null, min)\"\"\")\n",
    "    new_df = (df_min\n",
    "      .withColumn(\"new_key\", new_column_1)\n",
    "      .withColumn(\"new_val\", new_column_2)) \\\n",
    "    .na.drop() \\\n",
    "    .select(col(\"new_key\").alias(\"key\"), col(\"new_val\").alias(\"val\")) \\\n",
    "    .sort(\"val\", \"key\") \n",
    "        \n",
    "    df2 = new_df.distinct()\n",
    "    \n",
    "    return df2, df_min\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "begin = time.time()\n",
    "counter = 1\n",
    "iteration = 0\n",
    "while counter!=0:\n",
    "    iteration +=1\n",
    "    df1 = ccf_iterate_map_df(df)\n",
    "    df1.cache()\n",
    "    df.unpersist()\n",
    "    df, df_counter = ccf_iterate_reduce_df(df1)\n",
    "    df.cache()\n",
    "    df1.unpersist()\n",
    "    df_counter = df_counter.withColumn(\"counter_col\", expr(\"\"\"IF(min > key, 0, IF(min = val, 0, 1))\"\"\"))\n",
    "    counter = df_counter.select(sum(\"counter_col\")).collect()[0][0]\n",
    "    print(counter)\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as f\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class SparkDataManager:\n",
    "\n",
    "    def __init__(self, spark_context, initial_data_path, sep='\\\\s+', row_limit=None, skip_rows=None):\n",
    "        if initial_data_path is not None:\n",
    "            self.sql_context = pyspark.SQLContext(spark_context)\n",
    "            self.initial_data = self.sql_context.createDataFrame(\n",
    "                pd.read_csv(\n",
    "                    initial_data_path,\n",
    "                    sep=sep,\n",
    "                    nrows=row_limit,\n",
    "                    skiprows=skip_rows\n",
    "                ),\n",
    "                ['f', 't']\n",
    "            )\n",
    "            self.old_data = self.sql_context.createDataFrame(pd.DataFrame([[0, 1]], columns=['f', 't']))\n",
    "            self.data = self.initial_data\n",
    "            self.final_output = None\n",
    "        else:\n",
    "            raise ValueError(\"File path shouldn't be None.\")\n",
    "\n",
    "    def get_row_diff_and_update(self, new_data):\n",
    "        diff = np.abs(self.old_data.count() - new_data.count())\n",
    "        self.old_data = new_data\n",
    "        return diff\n",
    "\n",
    "    def show_data(self, full=False):\n",
    "        if full:\n",
    "            self.data.show(self.data.count(), False)\n",
    "        else:\n",
    "            self.data.show()\n",
    "\n",
    "    def save_input(self, file_name):\n",
    "        self.initial_data.toPandas().to_csv(f'out/spark_out/{file_name}', index=False)\n",
    "\n",
    "    def save_output(self, file_name=None):\n",
    "        if file_name is None:\n",
    "            file_name = 'output.csv'\n",
    "        if self.final_output:\n",
    "            self.final_output.toPandas().to_csv(f'./out/spark_out/{file_name}', index=False)\n",
    "\n",
    "    def load_output(self):\n",
    "        self.final_output = self.sql_context.createDataFrame(pd.read_csv('./out/spark_out/output.csv'))\n",
    "\n",
    "\n",
    "class SparkRunner:\n",
    "\n",
    "    def __init__(self, spark_context, data_path, sep='\\\\s+', row_limit=None, skip_rows=None):\n",
    "        self.data_path = data_path\n",
    "        if data_path is not None:\n",
    "            self.spark_context = spark_context\n",
    "            self.sdm = SparkDataManager(\n",
    "                spark_context=spark_context,\n",
    "                initial_data_path=data_path,\n",
    "                sep=sep,\n",
    "                row_limit=row_limit,\n",
    "                skip_rows=skip_rows\n",
    "            )\n",
    "            self.counter = 1\n",
    "            self.timer = 0.\n",
    "        else:\n",
    "            raise ValueError(\"File path shouldn't be None.\")\n",
    "\n",
    "    def map(self, data=None):\n",
    "        \"\"\"\n",
    "        For every edge A -> B, emits A -> B and B -> A\n",
    "        :param data: the data to use for the map. self.data is used by default.\n",
    "        I don't know if the parameter is useful, maybe for testing purposes\n",
    "        :return: The mapped data, flattened to be pretty and reconverted to DF to keep the same structure.\n",
    "        \"\"\"\n",
    "        if data is None:\n",
    "            data = self.sdm.data\n",
    "        # By doing a flatMap, we manage to emit both (from, to) and (to, from)\n",
    "        # The output of the map function here is then just the edged duplicated in both ways.\n",
    "        return data.rdd.flatMap(lambda tup: [(tup[0], tup[1]), (tup[1], tup[0])]).toDF(['f', 't'])\n",
    "\n",
    "    def reduce(self, data=None, verbose=False):\n",
    "        \"\"\"\n",
    "        # TODO Should this be broken down into functions ?\n",
    "        This is a bit more than just a reduce.\n",
    "        Steps performed in this function :\n",
    "            - Calls the MAP.\n",
    "            - Computes, for each 'from' node, the minimum 'to' node.\n",
    "            - Joins the grouped data with the map output.\n",
    "            - Computes the largest value between 'from' and 'min_t'\n",
    "            - Filters the edges A -> B with A > B\n",
    "            - Drops the duplicates.\n",
    "            - Emits the corresponding edge.\n",
    "            - Re-aggregates the data to find the minimum 'to' for each 'from'\n",
    "            - Emits the result.\n",
    "        :param data: The input data, by default, the output of self.map will be considered.\n",
    "        :param verbose: Should the intermediate steps be displayed.\n",
    "        :return: The output of the reduce steps.\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print('--- INPUT DATA ---')\n",
    "            self.sdm.show_data()\n",
    "        data = self.map(data)\n",
    "        grouped_data = data.groupBy('f') \\\n",
    "            .agg(f.min('t').alias('min_t'))\n",
    "        first_map_reduce_output = data.join(\n",
    "            grouped_data,\n",
    "            'f',\n",
    "            'inner'\n",
    "        )\n",
    "        if verbose:\n",
    "            print('--- MAP OUTPUT ---')\n",
    "            first_map_reduce_output.show(first_map_reduce_output.count(), False)\n",
    "        # TODO See if counter has to be used this way.\n",
    "        self.counter = self.sdm.get_row_diff_and_update(first_map_reduce_output)\n",
    "\n",
    "        reduce_out = grouped_data\\\n",
    "            .selectExpr('f as f', 'min_t as t').\\\n",
    "            union(\n",
    "                first_map_reduce_output.filter('f > min_t').selectExpr('t as f', 'min_t as t')\n",
    "            )\\\n",
    "            .sort(['f', 't']) \\\n",
    "            .dropDuplicates(['f', 't'])\\\n",
    "            .filter('f > t')\n",
    "\n",
    "        self.sdm.data = reduce_out\n",
    "\n",
    "        if verbose:\n",
    "            print('--- REDUCE OUTPUT ---')\n",
    "            # reduce_out.show(reduce_out.count(), False)\n",
    "            self.sdm.show_data()\n",
    "            print(f'Value of the counter : {self.counter}')\n",
    "        return reduce_out\n",
    "\n",
    "    def run(self, data=None, verbose=False):\n",
    "        \"\"\"\n",
    "        Runs the map-reduce algorithm until convergence\n",
    "        :param data: The data to use. If not provided, the file provided when creating the instance will be used.\n",
    "        :param verbose: Should the intermediate steps be displayed.\n",
    "        :return: The output of the last map-reduce algorithm, the connected components of the graph.\n",
    "        \"\"\"\n",
    "        reduce_out = None\n",
    "        start_time = time.time()\n",
    "        while self.counter > 0:\n",
    "            reduce_out = self.reduce(data, verbose=verbose)\n",
    "        self.sdm.final_output = reduce_out\n",
    "        self.timer = time.time() - start_time\n",
    "        return reduce_out\n",
    "\n",
    "    def plot_graph(self):\n",
    "        # Color palette used for graph\n",
    "        color_palette = ['red', 'green', 'blue', 'orange', 'purple', 'pink']\n",
    "        # Convert data to pandas for convenience.\n",
    "        pd_initial_data = self.sdm.initial_data.toPandas()\n",
    "        pd_output_data = self.sdm.final_output.toPandas()\n",
    "        # Getting nodes list\n",
    "        from_nodes = pd_initial_data.loc[:, 'f'].unique()\n",
    "        to_nodes = pd_initial_data.loc[:, 't'].unique()\n",
    "        all_nodes = list(set(list(from_nodes) + list(to_nodes)))\n",
    "        # Getting cluster for coloring\n",
    "        distinct_clusters = list(pd_output_data.loc[:, 't'].unique())\n",
    "        # Create a dictionary with structure {cluster : color}\n",
    "        colors = {}\n",
    "        palette_index = 0\n",
    "        for cluster in distinct_clusters:\n",
    "            colors[cluster] = color_palette[palette_index]\n",
    "            # Using modulo to have infinite color list.\n",
    "            palette_index = (palette_index + 1) % len(color_palette)\n",
    "        # Instantiate graph\n",
    "        graph = nx.Graph()\n",
    "        color_set = []\n",
    "        for node in all_nodes:\n",
    "            # Adding the node to the graph\n",
    "            graph.add_node(node)\n",
    "            # Adding the color\n",
    "            if node in distinct_clusters:\n",
    "                # If the node is a cluster than assign it's color\n",
    "                color_set.append(colors[node])\n",
    "            else:\n",
    "                # Else find the color of the cluster associated with the node.\n",
    "                row = pd_output_data[pd_output_data['f'] == node]\n",
    "                # This if is in case we don't find a color for the node.\n",
    "                if row.shape[0] > 0:\n",
    "                    cluster = row.values[0][1]\n",
    "                    color_set.append(colors[cluster])\n",
    "                else:\n",
    "                    color_set.append('grey')\n",
    "\n",
    "        # Append the edges.\n",
    "        for index, edge in pd_initial_data.iterrows():\n",
    "            graph.add_edge(edge[0], edge[1])\n",
    "\n",
    "        nx.draw(graph, node_color=color_set, with_labels=True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "659.0312879085541\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#sc = pyspark.SparkContext()\n",
    "\n",
    "run_list = []\n",
    "\n",
    "r2 = SparkRunner(sc, 'web-Google 2.txt', row_limit=10000000000000000, skip_rows=4)\n",
    "r2.run()\n",
    "run_list.append(r2.timer)\n",
    "print(r2.timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------------+\n",
      "|node|collect_list(neighbor)|\n",
      "+----+----------------------+\n",
      "| 148|  [218976, 272506, ...|\n",
      "| 463|  [437182, 486434, ...|\n",
      "| 471|      [896031, 900016]|\n",
      "| 833|      [654833, 816895]|\n",
      "|1088|              [567347]|\n",
      "|1238|  [147652, 331119, ...|\n",
      "|1342|  [185821, 844418, ...|\n",
      "|1580|      [274601, 852375]|\n",
      "|1591|              [791553]|\n",
      "|1645|  [361733, 438491, ...|\n",
      "|1829|  [43717, 90485, 96...|\n",
      "|1959|  [50732, 78369, 16...|\n",
      "|2142|  [200304, 251177, ...|\n",
      "|2366|  [44558, 277876, 6...|\n",
      "|2659|  [15587, 28725, 59...|\n",
      "|2866|              [749842]|\n",
      "|3175|  [120061, 323129, ...|\n",
      "|3749|              [653380]|\n",
      "|3794|       [79765, 765334]|\n",
      "|3997|               [17918]|\n",
      "+----+----------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "1 None\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 15.0 failed 1 times, most recent failure: Lost task 6.0 in stage 15.0 (TID 77) (hippolytes-macbook-pro.home executor driver): java.io.IOException: Cannot run program \"python3.6\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:209)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:132)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:105)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 16 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: Cannot run program \"python3.6\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:209)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:132)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:105)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 16 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-13f1246adafa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmin_fm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'node'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'neighbor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \"\"\"\n\u001b[1;32m    948\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 15.0 failed 1 times, most recent failure: Lost task 6.0 in stage 15.0 (TID 77) (hippolytes-macbook-pro.home executor driver): java.io.IOException: Cannot run program \"python3.6\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:209)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:132)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:105)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 16 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: Cannot run program \"python3.6\": error=2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:209)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:132)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:105)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 16 more\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"python3.6\"\n",
    "import pyspark\n",
    "#sc = pyspark.SparkContext()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)\n",
    "from pyspark.sql import functions as F\n",
    "import itertools\n",
    "\n",
    "def min_fm(x):\n",
    "    return [(a,x[0]) for a in x[1]]+[(x[0],a) for a in x[1]]\n",
    "\n",
    "data = spark.read.format(\"csv\").option(\"inferSchema\", \"true\")\\\n",
    "                                        .option(\"delimiter\", ',')\\\n",
    "                                        .option(\"header\", 'true')\\\n",
    "                                        .load(\"web-Google_total.csv\").toDF(\"node\",\"neighbor\").cache()\n",
    "data = data.groupby('node').agg(F.collect_list('neighbor'))\n",
    "print(\"1\", data.show())\n",
    "i = 0\n",
    "checker = []\n",
    "\n",
    "begin = time.time()\n",
    "\n",
    "while True:\n",
    "    data = data.rdd.map(tuple)\n",
    "    print(\"2\", data.collect())\n",
    "    data = data.flatMap(lambda x:min_fm(x)).toDF(['node','neighbor'])\n",
    "    print(\"3\", data)\n",
    "    data = data.groupby('node').agg(F.collect_set('neighbor'))\n",
    "    print(\"4\", data)\n",
    "    data = data.rdd.map(tuple)\n",
    "    print(\"5\", data)\n",
    "    data = data.map(lambda x: (min([x[0]]+x[1]),list(set(x[1])))).toDF(['node','neighbor'])\n",
    "    print(\"6\", data)\n",
    "    data = data.groupby('node').agg(F.collect_list('neighbor'))\n",
    "    print(\"7\", data)\n",
    "    data = data.rdd.map(tuple)\n",
    "    print(\"8\", data)\n",
    "    data = data.map(lambda x:(x[0],list(itertools.chain(*x[1])))).map(\n",
    "        lambda x: (min([x[0]]+x[1]),list(set(x[1])))).toDF(['node','neighbor'])\n",
    "    temp_count = data.count()\n",
    "    if i==0:\n",
    "        count = temp_count\n",
    "        i+=1\n",
    "    else:\n",
    "        if temp_count==count and len(checker)>1:\n",
    "            print('The number of connected components is',count)\n",
    "            break\n",
    "        elif temp_count==count:\n",
    "            print('so far',temp_count)\n",
    "            count = temp_count\n",
    "            checker.append(True)\n",
    "        else:\n",
    "            print('so far',temp_count)\n",
    "            count = temp_count\n",
    "\n",
    "#output_sentence = ''.join([str(count)])\n",
    "\n",
    "#end = time.time()\n",
    "\n",
    "#print(\"It took {} seconds\".format(end-begin))\n",
    "#with open('CCRes','w') as f:\n",
    "#    f.write(output_sentence)\n",
    "#    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[node: int, neighbor: int]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"csv\").option(\"inferSchema\", \"true\")\\\n",
    "                                        .option(\"delimiter\", ',')\\\n",
    "                                        .option(\"header\", 'true')\\\n",
    "                                        .load(\"web-Google_total.csv\").toDF(\"node\",\"neighbor\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processGraphRDD(filepath):\n",
    "    \n",
    "  \n",
    "    def reduce_ccf(x):\n",
    "        key = x[0]\n",
    "        values = x[1]\n",
    "        min_value = values.pop(values.index(min(values)))\n",
    "        ret = []\n",
    "        if min_value < key:\n",
    "            ret.append((key, min_value))\n",
    "            for value in values:\n",
    "                accum.add(1) # l on peu surement add en une fois la taille de la liste\n",
    "                ret.append((value, min_value)) # l on peut surement zip \n",
    "            return (ret)\n",
    "  \n",
    "  \n",
    "        text_file = sc.textFile(filepath)\n",
    "  \n",
    "        text_file = text_file.filter(lambda x: \"#\" not in x)\n",
    "        text_file_split = text_file.map(lambda x: x.split())\n",
    "        input = text_file_split.map(lambda x: (int(x[0]), int(x[1])))\n",
    "\n",
    "        accum = sc.accumulator(1)\n",
    "        while accum.value > 0:\n",
    "\n",
    "            accum.value = 0\n",
    "            print(\"Start loop \", accum.value)\n",
    "\n",
    "    # CCF-Iterate\n",
    "            it_map = input.flatMap(lambda x: ((x[0], x[1]), (x[1], x[0])))\n",
    "#     it_groupby = it_map.groupByKey().mapValues(lambda x: sorted(x))\n",
    "            it_groupby = it_map.groupByKey().mapValues(list)\n",
    "            it_reduce = it_groupby.flatMap(lambda x: reduce_ccf(x))\n",
    "    \n",
    "            # CCF-Dedup\n",
    "            ded_map = it_reduce.map(lambda x: ((x[0], x[1]), None))\n",
    "            ded_groupby = ded_map.groupByKey().mapValues(list)\n",
    "            input = ded_groupby.map(lambda x: (x[0][0], x[0][1]))\n",
    "\n",
    "            viz = input.collect()\n",
    "            print('-------------')\n",
    "\n",
    "        print(\"Processed file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "processGraphRDD(\"web-Google 2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
